###### Hadoop ######
--Start hadoop:
start-dfs.sh
start-yarn.sh

--To see hadoop contents:
hadoop fs -ls /

hdfs dfs -rm -r /CA1_S2
hadoop fs -mkdir /CA1_S2
hadoop fs -put ./people_increased.csv /CA1_S2
hdfs fsck hdfs:///CA1_S2/people_increased.csv -files -blocks -locations


hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar -mapper ./mapper.py -reducer ./reducer.py -input /CA2_S2/CleanProjectTweets.csv -output /CA2_S2_MapReduceOutput

hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar -mapper ./mapperc.py -reducer ./reducerc.py -input /CA2_S2/CleanProjectTweets.csv -output /CA2_S2_tweetsclean

hadoop fs -rm -r /CA2_S2_MapReduceOutput

hadoop fs -chmod -R 777 /CA2_S2/
hadoop fs -rm -r /CA2_S2/CleanProjectTweetsCleaned.csv

####### MySQL #######

mysql -u root -p

show databases;

use BenchTest;

show tables;

delete from usertable;

####### Cassandra #########
# To find out cassandra IP:
hostname -I

--Firing up Cassandra, let it running:
cd /usr/local/cassandra/
bin/cassandra -f


--In a different terminal:
cd /usr/local/cassandra
bin/cqlsh

--See keyspaces:
DESCRIBE KEYSPACES;

--Creating twitterdb
CREATE KEYSPACE twitterdb WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };

cqlsh> use twitterdb;

CREATE TABLE tweets (ids text, tweet_id text, date text, flag text, user text, text text, PRIMARY KEY (ids));

DESCRIBE table tweets;

COPY twitterdb.tweets (ids, tweet_id, date, flag, user, text) 
FROM '/home/hduser/Desktop/CA2_S2/CleanProjectTweets.csv' 
WITH HEADER = true AND delimiter = ',';

CREATE TABLE tweets(ids text, tweet_id text, date text, flag text, user text, text text, primary key (ids, tweet_id, date, flag, user, text));

COPY tweets FROM '/home/hduser/Desktop/CA2_S2/CleanTweets.csv'  WITH HEADER = TRUE AND DELIMITER = ',';

SELECT * FROM twitter LIMIT 100;

--To delete records:
TRUNCATE usertable;

--To count records:
SELECT count(*) FROM usertable;

##### Mongodb #######

mongod --bind_ip 127.0.0.1
--Let this command running in a separate window.

Type below command in a new window:
mongo
show dbs
use ycsb
show collections
db.usertable.count()
db.usertable.find().limit(10)
db.dropDatabase()

--Setting up ycsb:

-First create this table:

create keyspace ycsb WITH REPLICATION = {'class' : 'SimpleStrategy', 'replication_factor': 3 };

USE ycsb;

create table usertable (y_id varchar primary key,field0 varchar,field1 varchar,field2 varchar,field3 varchar,field4 varchar,field5 varchar,field6 varchar,field7 varchar,field8 varchar,field9 varchar);

select * from usertable limit 10;
select count(*) from usertable;

--To delete records:
TRUNCATE usertable;

###### YCSB #######

cd /home/hduser/ycsb-0.17.0
--workloads, to inspect the various workloads
cd workloads

## MySQL ##
--Uploading Records to MySQL:
./bin/ycsb.sh load jdbc -P ./jdbc-binding/conf/db.properties -P workloads/workloada

--Running workloads a, b, c, d, e, f and g for MySQL:
./bin/ycsb.sh run jdbc -P workloads/workloada -P ./jdbc-binding/conf/db.properties
./bin/ycsb.sh run jdbc -P workloads/workloadb -P ./jdbc-binding/conf/db.properties
./bin/ycsb.sh run jdbc -P workloads/workloadc -P ./jdbc-binding/conf/db.properties
./bin/ycsb.sh run jdbc -P workloads/workloadd -P ./jdbc-binding/conf/db.properties
./bin/ycsb.sh run jdbc -P workloads/workloade -P ./jdbc-binding/conf/db.properties
./bin/ycsb.sh run jdbc -P workloads/workloadf -P ./jdbc-binding/conf/db.properties
./bin/ycsb.sh run jdbc -P workloads/workloadg -P ./jdbc-binding/conf/db.properties

## Mongodb ##
--Uploading Records to Mongodb:
./bin/ycsb.sh load mongodb -s -P workloads/workloada

--Running workloads a, b, c, d, e, f and g for MongoDB:
./bin/ycsb.sh run mongodb -s -P workloads/workloada
./bin/ycsb.sh run mongodb -s -P workloads/workloadb
./bin/ycsb.sh run mongodb -s -P workloads/workloadc
./bin/ycsb.sh run mongodb -s -P workloads/workloadd
./bin/ycsb.sh run mongodb -s -P workloads/workloade
./bin/ycsb.sh run mongodb -s -P workloads/workloadf
./bin/ycsb.sh run mongodb -s -P workloads/workloadg

## Cassandra ##

--Run below command to load records
./bin/ycsb.sh load cassandra-cql -p hosts=10.0.2.15,127.0.0.1 -p port=9042 -p debug=true -P workloads/workloada -s

--Running workloads a, b, c and g for Cassandra:

./bin/ycsb.sh run cassandra-cql -p hosts=10.0.2.15,127.0.0.1 -p port=9042 -p debug=true -P workloads/workloada -s
./bin/ycsb.sh run cassandra-cql -p hosts=10.0.2.15,127.0.0.1 -p port=9042 -p debug=true -P workloads/workloadb -s
./bin/ycsb.sh run cassandra-cql -p hosts=10.0.2.15,127.0.0.1 -p port=9042 -p debug=true -P workloads/workloadc -s
./bin/ycsb.sh run cassandra-cql -p hosts=10.0.2.15,127.0.0.1 -p port=9042 -p debug=true -P workloads/workloadd -s
./bin/ycsb.sh run cassandra-cql -p hosts=10.0.2.15,127.0.0.1 -p port=9042 -p debug=true -P workloads/workloade -s
./bin/ycsb.sh run cassandra-cql -p hosts=10.0.2.15,127.0.0.1 -p port=9042 -p debug=true -P workloads/workloadf -s
./bin/ycsb.sh run cassandra-cql -p hosts=10.0.2.15,127.0.0.1 -p port=9042 -p debug=true -P workloads/workloadg -s


#### Hive #### Use hive to demonstrate map reduce along the mapreduce scripts I have.
CREATE TABLE IF NOT EXISTS tableHive (numRow int, name String, city String, county String, country String) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\054'
LOAD DATA INPATH 'hdfs://localhost:9000/hive/pig_tutorial_sample.txt' INTO TABLE tableHive;
hive>SELECT * FROM tableHive;
--export output:

INSERT OVERWRITE LOCAL DIRECTORY '/home/hduser/export' ROW
FORMAT DELIMITED FIELDS TERMINATED BY ',' SELECT * FROM tableHive;

INSERT OVERWRITE DIRECTORY
'hdfs://localhost:9000/hive/data/output/export' ROW FORMAT
DELIMITED FIELDS TERMINATED BY ',' SELECT * FROM tableHive;

hadoop fs -cat /hive/data/output/export/*




















